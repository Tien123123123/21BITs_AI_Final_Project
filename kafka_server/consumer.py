import sys, os
from kafka import KafkaConsumer, TopicPartition
from evaluation_pretrain.pretrain_contentbase import pretrain_contentbase
from evaluation_pretrain.pretrain_collaborative import pretrain_collaborative
from arg_parse.arg_parse_contentbase import arg_parse_contentbase
from arg_parse.arg_parse_collaborative import arg_parse_collaborative
import logging
from qdrant_server.load_data import load_to_df
from process_data.preprocessing import preprocess_data
from qdrant_server.server import connect_qdrant
import json
from datetime import datetime

sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "../")))

def kafka_consumer(topic_name, bootstrap_servers='kafka.d2f.io.vn:9092'):
    # Kh·ªüi t·∫°o logging
    logging.basicConfig(level=logging.INFO)
    logging.info(f"Command-line arguments: {sys.argv}")

    try:
        consumer = KafkaConsumer(
            topic_name,  # Subscribe topic tr·ª±c ti·∫øp nh∆∞ code c≈©
            bootstrap_servers=bootstrap_servers,
            auto_offset_reset='latest',  # B·∫Øt ƒë·∫ßu t·ª´ cu·ªëi n·∫øu kh√¥ng c√≥ offset commit
            enable_auto_commit=False,  # Commit th·ªß c√¥ng ƒë·ªÉ ki·ªÉm so√°t
            group_id='my-consumer-group-' + str(datetime.utcnow().timestamp()),  # Group ID duy nh·∫•t m·ªói l·∫ßn ch·∫°y
            value_deserializer=lambda v: v.decode('utf-8') if v else None
        )

        logging.info(f"üì° ƒêang l·∫Øng nghe topic: {topic_name} ƒë·ªÉ ƒë·ª£i message m·ªõi...")

        # L·∫•y offset cu·ªëi c√πng khi kh·ªüi ƒë·ªông
        consumer.poll(timeout_ms=1000)  # Poll ƒë·∫ßu ti√™n ƒë·ªÉ l·∫•y metadata
        initial_offsets = {TopicPartition(topic_name, p): consumer.position(TopicPartition(topic_name, p))
                          for p in consumer.partitions_for_topic(topic_name)}
        for tp, offset in initial_offsets.items():
            logging.info(f"Partition {tp.partition} - Offset cu·ªëi c√πng khi kh·ªüi ƒë·ªông: {offset}")

        # L·∫Øng nghe message m·ªõi
        for message in consumer:
            if message.value is not None:
                current_offset = message.offset
                tp = TopicPartition(message.topic, message.partition)

                # Ch·ªâ x·ª≠ l√Ω n·∫øu offset l·ªõn h∆°n offset kh·ªüi ƒë·∫ßu
                if current_offset >= initial_offsets[tp]:
                    logging.info(f"üì¢ Nh·∫≠n ƒë∆∞·ª£c message m·ªõi: {message.value}")
                    logging.info(f"  - Topic: {message.topic}")
                    logging.info(f"  - Partition: {message.partition}")
                    logging.info(f"  - Offset: {message.offset}")
                    logging.info(f"  - Key: {message.key}")
                    logging.info(f"  - Status: Ready to Pretrain!")

                    # Ki·ªÉm tra th·ªùi gian message (t√πy ch·ªçn)
                    try:
                        msg_data = json.loads(message.value)
                        event_time = msg_data.get('event_time')
                        if event_time:
                            event_dt = datetime.strptime(event_time, "%Y-%m-%d %H:%M:%S UTC")
                            now = datetime.utcnow()
                            if (now - event_dt).total_seconds() > 3600:  # B·ªè qua message c≈© h∆°n 1 gi·ªù
                                logging.info(f"Message t·∫°i offset {current_offset} qu√° c≈© ({event_time}), b·ªè qua.")
                                consumer.commit()
                                continue
                    except (json.JSONDecodeError, ValueError) as e:
                        logging.warning(f"Kh√¥ng th·ªÉ parse event_time t·ª´ message: {e}")

                    # K·∫øt n·ªëi Qdrant v√† x·ª≠ l√Ω d·ªØ li·ªáu
                    q_drant_end_point = "http://103.155.161.100:6333"
                    q_drant_collection_name = "recommendation_system"
                    client = connect_qdrant(end_point=q_drant_end_point, collection_name=q_drant_collection_name)
                    df = load_to_df(client=client, collection_name=q_drant_collection_name)
                    df = preprocess_data(df, is_encoded=False, nrows=None)
                    pretrain_collaborative(args=arg_parse_collaborative(), df=df)
                    pretrain_contentbase(args=arg_parse_contentbase(), df=df)

                    logging.info("-" * 50)
                    consumer.commit()  # Commit offset sau khi x·ª≠ l√Ω
                    initial_offsets[tp] = current_offset + 1  # C·∫≠p nh·∫≠t offset kh·ªüi ƒë·∫ßu
                else:
                    logging.debug(f"Offset {current_offset} kh√¥ng ph·∫£i message m·ªõi, b·ªè qua.")
                    consumer.commit()
            else:
                logging.info("üì¢ Message r·ªóng ho·∫∑c kh√¥ng c√≥ d·ªØ li·ªáu.")
                consumer.commit()

    except Exception as e:
        logging.error(f"‚ùå L·ªói khi k·∫øt n·ªëi ho·∫∑c nh·∫≠n message: {e}")

if __name__ == "__main__":
    kafka_consumer(
        topic_name="model_retrain_event",
        bootstrap_servers="kafka.d2f.io.vn:9092"
    )